{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Twitter POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Jiyu Chen\n",
    "\n",
    "Student ID: 908066\n",
    "\n",
    "Python version used: 3.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: 11pm, Sunday April 15\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be adapting a POS tagger to Twitter data, starting from a tagger trained on Penn Treebank. You will also use prior information on the Twitter tagset to obtain better performance. Finally, you will also analyse your results in a more fine-grained way. For extra credits, you will implement the Expectation-Maximisation algorithm.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Preprocessing (2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: you first task is to preprocess the data. We will use two datasets for training: 1) the Penn Treebank sample used in the workshops and 2) the Twitter samples data you used in Homework 1. In order to adapt the tagger to the Twitter data we need to built a *joint* vocabulary containing all the word types in PTB and the twitter_samples corpora. So, in addition to preprocessing, your code should also build this vocabulary. Finally, you should also store the tagset, for reasons that will be clearer later.\n",
    "\n",
    "<b>Important</b>: you are allowed to reuse all the code from the workshop notebooks. In fact you are encouraged to do so as much of this homework will be based on these notebooks.\n",
    "\n",
    "The vocabulary and the tagset should be stored in Python dictionaries, mapping each word (or tag) to an index (integer). This is similar to what is done in the W6/W7 workshop notebooks. The preprocessed corpora should contain indices only, as in the workshop.\n",
    "\n",
    "Let's start with the PTB data. You should iterate over all sentences and words, and build the vocabulary and the tagset. Important: make sure you <b>lowercase</b> words before they are added to the dictionary. You should also generate the preprocessed corpus. It should be a list where each element is a tagged sentence, represented as another list of (word, tag) indices (which should correspond to the original words/tags). Print the first preprocessed sentence, the index for the word 'electricity' and the length of the full tagset. (0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (2, 1), (3, 2), (4, 3), (5, 4), (2, 1), (6, 5), (7, 6), (8, 7), (9, 8), (10, 9), (11, 7), (12, 4), (13, 8), (14, 0), (15, 2), (16, 10)]\n",
      "1095\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "#generate preprocessed corpus\n",
    "\n",
    "# reference from WSTA_N7_unsupervised_HMMs\n",
    "def preprocessing():\n",
    "    \"\"\"create word and tag list and transform corpus\n",
    "    into a corpus of (word,tag) pair list\n",
    "    return: transformed corpus, word index list, tag index list\n",
    "    \"\"\"\n",
    "    corpus = treebank.tagged_sents()\n",
    "    word_numbers = {}\n",
    "    tag_numbers = {}\n",
    "    num_corpus = []\n",
    "    for sent in corpus:\n",
    "        num_sent = []\n",
    "        for word, tag in sent:\n",
    "            wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "            ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "            num_sent.append((wi, ti))\n",
    "        num_corpus.append(num_sent)\n",
    "    return num_corpus,word_numbers,tag_numbers\n",
    "\n",
    "#end of reference\n",
    "        \n",
    "        \n",
    "corpus,volcabulary,tagset = preprocessing()\n",
    "print(corpus[0])\n",
    "print(volcabulary.get('electricity'))\n",
    "print(len(tagset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now you should do the same with the twitter_samples dataset. From now on, we will refer this dataset as the **training** tweets. Since this data is not tagged, the preprocessed corpus should be a list where each element is another list containing indices only (instead of (word, tag) tuples). A tokenised version of twitter_samples is available through the method .tokenized(), use this method to read your corpus. Besides generating the corpus, you should also **update** the vocabulary with the new words from this corpus.\n",
    "\n",
    "There are two things to keep in mind when doing this process:\n",
    "\n",
    "1) We will perform a bit more of preprocessing in this dataset, besides lowercasing. Specifically, you should replace special tokens with special symbols, as follows:\n",
    "- Username mentions are tokens that start with '@': replace these tokens with 'USER_TOKEN'\n",
    "- Hashtags are tokens that start with '#': replace these with 'HASHTAG_TOKEN'\n",
    "- Retweets are represented as the token 'RT' (or 'rt' if you lowercase first): replace these with 'RETWEET_TOKEN'\n",
    "- URLs are tokens that start with 'https://' or 'http://': replace these with 'URL_TOKEN'\n",
    "\n",
    "2) **Do not create a new vocabulary**. Instead, you should update the vocabulary built from PTB with any new words present in this corpus. These should *include* the special tokens defined above but *not* the original un-preprocessed tokens.\n",
    "\n",
    "The easiest way to do these steps is by doing 3 passes over the data: preprocess the words first, update the vocabulary and finally convert the corpus into the list format described above. However, it is possible to do all of this in one pass only.\n",
    "\n",
    "Print the first sentence from your preprocessed corpora, the index for the word 'electricity' and the index for 'HASHTAG_TOKEN'. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "11409\n",
      "[11387, 182, 11388, 11389]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "import re\n",
    "\n",
    "\n",
    "def twitter_process(words):\n",
    "    \"\"\"transform twitter corpus into (word,tag)\n",
    "    pair index list and also add new word into word list\n",
    "    return: transformed twitter, updated word index list\n",
    "    \"\"\"\n",
    "    corpus = twitter_samples.tokenized()\n",
    "    word_numbers = words\n",
    "    num_corpus = []\n",
    "    \n",
    "    #special replacement using regex\n",
    "    user_token = re.compile('^@.*')\n",
    "    hashtag_token = re.compile('^#.*')\n",
    "    retweet_token = re.compile('^rt$')\n",
    "    url_token = re.compile('^http[s]?://.*')\n",
    "    \n",
    "    for sent in corpus:\n",
    "        num_sent = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if user_token.match(word):\n",
    "                word = user_token.sub(\"USER_TOKEN\",word)\n",
    "            if hashtag_token.match(word):\n",
    "                word = hashtag_token.sub(\"HASHTAG_TOKEN\",word)\n",
    "            if retweet_token.match(word):\n",
    "                word = retweet_token.sub(\"RETWEET_TOKEN\",word)\n",
    "            if url_token.match(word):\n",
    "                word = url_token.sub(\"URL_TOKEN\",word) \n",
    "            wi = word_numbers.setdefault(word, len(word_numbers))\n",
    "            num_sent.append(wi)\n",
    "        num_corpus.append(num_sent)\n",
    "        \n",
    "    return num_corpus,word_numbers\n",
    "\n",
    "   \n",
    "twitter_corpus,volcabulary= twitter_process(volcabulary)\n",
    "print(volcabulary.get('electricity'))\n",
    "print(volcabulary.get('HASHTAG_TOKEN'))\n",
    "print(twitter_corpus[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions:</b> now we will preprocess the tagged twitter corpus used in W7 (Ritter et al.). This dataset will be referred from now on as **test** tweets. Before you do that though, you should update the tagset.\n",
    "\n",
    "You might have noticed this in the workshop but this dataset has a few extra tags, besides the PTB ones. These were added to incorporate specific phenomena that happens on Twitter:\n",
    "- \"USR\": username mentions\n",
    "- \"HT\": hashtags\n",
    "- \"RT\": retweets\n",
    "- \"URL\": URL addresses\n",
    "\n",
    "Notice that these special tags correspond to the special tokens we preprocessed before. These steps will be important in Part 3 later.\n",
    "\n",
    "There a few additional tags which are not specific to Twitter but are not present in the PTB sample:\n",
    "- \"VPP\"\n",
    "- \"TD\"\n",
    "- \"O\"\n",
    "\n",
    "You should add these new seven tags to the tagset you built when reading the PTB corpus.\n",
    "\n",
    "Another task is to add an extra type to the vocabulary: `<unk>`. This is in order to account for unknown or out-of-vocabulary words.\n",
    "\n",
    "Finally, build two \"inverted indices\" for the vocabulary and the tagset. These should be lists, where the \"i\"-th element should contain the word (or tag) corresponding to the index \"i\" in the vocabulary (or tagset).\n",
    "\n",
    "After doing these tasks, print the index for `<unk>` and the length of your resulting tagset. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "26069\n"
     ]
    }
   ],
   "source": [
    "def pendingTagset(tagset):\n",
    "    \"\"\"pend 7 special tags into tagset\n",
    "    return: updated tagset index list\n",
    "    \"\"\"\n",
    "    tagset.setdefault(\"USR\", len(tagset))\n",
    "    tagset.setdefault(\"HT\", len(tagset))\n",
    "    tagset.setdefault(\"RT\", len(tagset))\n",
    "    tagset.setdefault(\"URL\", len(tagset))\n",
    "    tagset.setdefault(\"VPP\", len(tagset))\n",
    "    tagset.setdefault(\"TD\", len(tagset))\n",
    "    tagset.setdefault(\"O\", len(tagset))\n",
    "    return tagset\n",
    "\n",
    "#print(tagset)\n",
    "tagset = pendingTagset(tagset)\n",
    "\n",
    "#add special volcabulary into word index list\n",
    "volcabulary.setdefault('<unk>',len(volcabulary))\n",
    "\n",
    "def invert(data):\n",
    "    \"\"\"invert index list\n",
    "    invert [{index:item}]\n",
    "    to [item] ordered by it's index\n",
    "    return: inverted list\n",
    "    \"\"\"\n",
    "    index_value = {}\n",
    "    inverted_list = []\n",
    "    print\n",
    "    for w in list(data):\n",
    "        index_value.setdefault(data.get(w),w)\n",
    "    \n",
    "    for index in list(index_value):\n",
    "        inverted_list.append(index_value.get(index))\n",
    "    \n",
    "    return inverted_list\n",
    "        \n",
    "inv_volcabulary = invert(volcabulary)\n",
    "inv_tagset = invert(tagset)\n",
    "print(len(inv_tagset))\n",
    "print(volcabulary.get('<unk>'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now we can read the test tweets. Store them in the same format as the PTB corpora (list of lists containing (word, tag) index tuples). Do the same preprocessing steps that you did for the training tweets (lowercasing + replace special tokens). However, **do not** update the vocabulary. Why? Because the test set should simulate a real-world scenario, where out-of-vocabulary words can appear. Instead, after preprocessing each word, you should check if that word is in the vocabulary. If yes, just replace it with its index, otherwise you should replace it with the index for the `<unk>` token. Remember: you can reuse the code from the workshop for this task. Just be mindful that in the workshop we stored words and tags in two separate lists: here you should have a single list, as in the PTB corpus you preprocessed above.\n",
    "\n",
    "When reading the POS tags for the test tweets you should do some additional preprocessing. There are three tags in this dataset which correspond to PTB tags but are represented with different names:\n",
    "- \"(\". In PTB, this is represented as \"-LRB-\"\n",
    "- \")\". In PTB, this is represented as \"-RRB-\"\n",
    "- \"NONE\". In PTB, this is represented as \"-NONE-\"\n",
    "\n",
    "As you build the corpus for the test tweets, you should check if the tag for a word is one of the above. If yes, you should use the PTB equivalent instead. In practice, it is sufficient to ensure you use the correct index for the corresponding tag, using your tagset dictionary. This concept is sometimes referred as *tag harmonisation*, where two different tagsets are mapped to each other.\n",
    "\n",
    "After this, print the first sentence of your preprocessed corpus. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11392, 46), (61, 19), (114, 11), (8, 7), (3224, 8), (170, 9), (325, 33), (1325, 19), (2375, 22), (3205, 12), (182, 9), (799, 2), (1522, 3), (16, 10), (8490, 0), (1146, 0), (2495, 0), (14039, 43), (26069, 0), (16, 10), (4263, 17), (1760, 4), (9464, 8), (2259, 17), (888, 4), (741, 8), (16, 10)]\n",
      "[('USER_TOKEN', 'USR'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('the', 'DT'), ('view', 'NN'), ('from', 'IN'), ('where', 'WRB'), ('i', 'PRP'), (\"'m\", 'VBP'), ('living', 'VBG'), ('for', 'IN'), ('two', 'CD'), ('weeks', 'NNS'), ('.', '.'), ('empire', 'NNP'), ('state', 'NNP'), ('building', 'NNP'), ('=', 'SYM'), ('<unk>', 'NNP'), ('.', '.'), ('pretty', 'RB'), ('bad', 'JJ'), ('storm', 'NN'), ('here', 'RB'), ('last', 'JJ'), ('evening', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "except: # Python 2\n",
    "    urllib.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "    \n",
    "    \n",
    "def test_preprocess():\n",
    "    \"\"\"transform the test corpus into \n",
    "    (word,tag) index list by refering \n",
    "    the training data\n",
    "    return: transformed test corpus\n",
    "    \"\"\"\n",
    "    user_token = re.compile('^@.*')\n",
    "    hashtag_token = re.compile('^#.*')\n",
    "    retweet_token = re.compile('^rt$')\n",
    "    url_token = re.compile('^http[s]?://.*')\n",
    "    lrb = re.compile('^\\($')\n",
    "    rrb = re.compile('^\\)$')\n",
    "    none = re.compile('^NONE$')\n",
    "    corpus = []\n",
    "    sent = []\n",
    "    with open('pos.txt') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                corpus.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                word, pos = line.strip().split()\n",
    "                word = word.lower()\n",
    "                if user_token.match(word):\n",
    "                    word = user_token.sub(\"USER_TOKEN\",word)\n",
    "                if hashtag_token.match(word):\n",
    "                    word = hashtag_token.sub(\"HASHTAG_TOKEN\",word)\n",
    "                if retweet_token.match(word):\n",
    "                    word = retweet_token.sub(\"RETWEET_TOKEN\",word)\n",
    "                if url_token.match(word):\n",
    "                    word = url_token.sub(\"URL_TOKEN\",word)\n",
    "                if lrb.match(pos):\n",
    "                    pos = lrb.sub(\"-LRB-\",pos)\n",
    "                if rrb.match(pos):\n",
    "                    pos = rrb.sub(\"-RRB-\",pos)\n",
    "                if none.match(pos):\n",
    "                    pos = none.sub(\"-NONE-\",pos)\n",
    "                if volcabulary.get(word) == None:\n",
    "                    word = '<unk>'\n",
    "                sent.append((volcabulary.get(word),tagset.get(pos)))\n",
    "    #print(corpus[0])\n",
    "    return corpus\n",
    "    \n",
    "test_corpus = test_preprocess()\n",
    "\n",
    "#print out the first indexed sentence of transformed corpus\n",
    "print(test_corpus[0])\n",
    "\n",
    "\n",
    "#print out the first context sentence of transformed corpus\n",
    "text = []\n",
    "for (w,t) in test_corpus[0]:\n",
    "    text.append((inv_volcabulary[w],inv_tagset[t]))\n",
    "print(text)\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hint</b>: if you did these steps correctly you should have 53 tags in your tagset and around 26000 words in your vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Running the PTB tagger on the test tweets (1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: your next task is to train a POS tagger on the PTB data and try it on the test tweets. This is exactly what we did in W7: feel free to reuse code. However, we are also gonna modify the code a bit.\n",
    "\n",
    "Your first task is encapsulate the HMM training code into a function. You should name your function `count`. This function should take these input parameters:\n",
    "- A tagged corpus, in the format described above (list of lists containing (word, tag) index tuples).\n",
    "- The vocabulary (a dict).\n",
    "- The tagset (a dict).\n",
    "\n",
    "Output return values should contain:\n",
    "- The initial tag probabilities (a vector).\n",
    "- The transition probabilities (a matrix).\n",
    "- The emission probabilities (a matrix).\n",
    "\n",
    "Notice that in the workshop code the vocabulary and tagset were built as part of the training process. Here you should pass them explicitly as parameters instead. This is to ensure our tagger can take into account the words in the training tweets and the extra tags. Important: the workshop code initialise the probabilities with an `eps` value, to ensure you end up with non-zero probabilities for unseen events. You should do the same here.\n",
    "\n",
    "After writing your function, run it on the PTB corpus to obtain the initial, transition and emission probabilities. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count(tagset,volcabulary,corpus):\n",
    "    \"\"\"construct HMM and create\n",
    "    initial states, transition matrix, emission matrix\n",
    "    para: tagset: tagset index list\n",
    "    para: volcabulary: word index list\n",
    "    para: corpus: index transformed corpus\n",
    "    return: initial states, transition matrix, emission matrix\n",
    "    \"\"\"\n",
    "    S = len(tagset)\n",
    "    V = len(volcabulary)\n",
    "    num_corpus = corpus\n",
    "    \n",
    "    #reference from WSTA_N7_unsupervised_HMMs \n",
    "    # initalise\n",
    "    eps = 0.1\n",
    "    #initial state\n",
    "    pi = eps * np.ones(S)\n",
    "    #transition\n",
    "    A = eps * np.ones((S, S))\n",
    "    #emission\n",
    "    O = eps * np.ones((S, V))\n",
    "\n",
    "    # count\n",
    "    for sent in num_corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            #count emissions \n",
    "            O[tag, word] += 1\n",
    "            #using the first tag as initial state\n",
    "            if last_tag == None:\n",
    "                pi[tag] += 1\n",
    "            else:\n",
    "                #count transitions\n",
    "                A[last_tag, tag] += 1\n",
    "            #shift to next tag in HMM\n",
    "            last_tag = tag\n",
    "        \n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:]) \n",
    "    return pi, A, O\n",
    "#end of reference\n",
    "\n",
    "pi, A, O = count(tagset,volcabulary,corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now you should write a function for Viterbi. The input parameters are the same as in the workshop:\n",
    "- The parameters (probabilities) of your HMM (a tuple (initial, transition, emission)).\n",
    "- The input words (a list with numbers).\n",
    "\n",
    "The output is slightly different though:\n",
    "- A list of (word, tag) indices, containing the original input word and the predicted tag.\n",
    "\n",
    "Run Viterbi on the test tweets and store the predictions in a list (might take a few seconds). Remember that in the processing part you stored the test tweets as (word, tag) indices lists: make sure your input to Viterbi are word index lists only. Print the first sentence of your predicted list. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11392, 27), (61, 19), (114, 11), (8, 7), (3224, 8), (170, 9), (325, 33), (1325, 19), (2375, 22), (3205, 12), (182, 9), (799, 2), (1522, 3), (16, 10), (8490, 29), (1146, 8), (2495, 8), (14039, 10), (26069, 38), (16, 10), (4263, 29), (1760, 4), (9464, 8), (2259, 17), (888, 4), (741, 8), (16, 10)]\n"
     ]
    }
   ],
   "source": [
    "#reference from WSTA_N7_unsupervised_HMMs \n",
    "def viterbi(params, observations):\n",
    "    \"\"\"HMM prediction\n",
    "    para: params: HMM parameters\n",
    "    para: observations: word indexed list that waited to be tagged\n",
    "    return: (word,tag) indexed list\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "    predict =  list(reversed(ss))\n",
    "    for i in range(len(predict)):\n",
    "        prediction.append((observations[i],predict[i]))\n",
    "    return prediction\n",
    "\n",
    "# end of reference\n",
    "    \n",
    "def tagging(corpus,pi,transition,emission):\n",
    "    \"\"\"HMM predict by sentences\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    for sent in corpus:\n",
    "        sents = []\n",
    "        for (w,t) in sent:\n",
    "            sents.append(w)\n",
    "        pred = viterbi((pi,transition,emission),sents)\n",
    "        prediction.append(pred)\n",
    "    return prediction\n",
    "\n",
    "prediction = tagging(test_corpus,pi,A,O)\n",
    "print(prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: you should now evaluate the results. Write a function that takes (word, tag) lists as inputs and outputs the tag sequence using the original tags in the tagset. Your inputs should be a sentence and the tag inverted index you built before.\n",
    "\n",
    "Run this function on the predictions you obtained above **and** the test tweets, storing them in two separate lists. Finally, flat your predictions into a single list and do the same for the test tweets and report accuracy. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6371419163648337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def transform_prediction(corpus):\n",
    "    \"\"\"extract the tags indexes from a (word,tag) corpus\n",
    "    para: (word,tag) index format corpus\n",
    "    return: list of tag index\n",
    "    \"\"\"\n",
    "    tag_sequence = []\n",
    "    for sent in corpus:\n",
    "        for (w,t) in sent:\n",
    "            tag_sequence.append(t)\n",
    "    return tag_sequence\n",
    "\n",
    "gold_sequence = transform_prediction(test_corpus)\n",
    "pred_sequence = transform_prediction(prediction)\n",
    "\n",
    "def evaluation(seq1,seq2):\n",
    "    \"\"\"calculate accuracy of seq2 by refering to seq1\n",
    "    para: seq1: list of gold standard tag index\n",
    "    para: seq2: list of predicted tag index\n",
    "    return: accuracy\n",
    "    \"\"\"\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for i in range(len(seq1)):\n",
    "        if seq1[i] != seq2[i]:\n",
    "            error +=1\n",
    "        total += 1\n",
    "    accuracy = 1 - error/total\n",
    "    return accuracy\n",
    "        \n",
    "accuracy = evaluation(gold_sequence, pred_sequence)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Adapting the tagger using prior information (1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now your task is to adapt the tagger using prior information. What do we mean by that? Remember from part 1 that the twitter tagset has some extra tags, related to special tokens such as mentions and hashtags. In other words, **we know beforehand** that these special tokens **should** have these tags. However, because these tags never appear in the PTB data, the tagger has no such information. We are going to add this in order to improve the tagger.\n",
    "\n",
    "To recap, we know these things about the twitter data:\n",
    "- username mentions should have the tag 'USR'\n",
    "- hashtags should have the tag 'HT'\n",
    "- retweet tokens should have the tag 'RT'\n",
    "- URL tokens should have the tag 'URL'\n",
    "\n",
    "Remember how we replace these tokens with unique special ones (such as 'USER_TOKEN')? Your task is to adapt the emission probabilities for these tokens. Modify the emission matrix: assign 1.0 probability for the emission P('USER_TOKEN'|'USR') and 0.0 for P(word|'USR') for all other words. Do the same for the other three special tags.\n",
    "\n",
    "In order to do that, you should use the vocabulary and tagset dictionaries in order to obtain the indices for the corresponding words and tags. Then, use the indices to find the values in the emission matrix and modify them. Print your new emission matrix. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.15369893e-05 1.74752434e-04 8.32154448e-06 ... 8.32154448e-06\n",
      "  8.32154448e-06 8.32154448e-06]\n",
      " [1.33457894e-05 1.33457894e-05 6.51955158e-01 ... 1.33457894e-05\n",
      "  1.33457894e-05 1.33457894e-05]\n",
      " [1.62522347e-05 1.62522347e-05 1.62522347e-05 ... 1.62522347e-05\n",
      "  1.62522347e-05 1.62522347e-05]\n",
      " ...\n",
      " [3.83582662e-05 3.83582662e-05 3.83582662e-05 ... 3.83582662e-05\n",
      "  3.83582662e-05 3.83582662e-05]\n",
      " [3.83582662e-05 3.83582662e-05 3.83582662e-05 ... 3.83582662e-05\n",
      "  3.83582662e-05 3.83582662e-05]\n",
      " [3.83582662e-05 3.83582662e-05 3.83582662e-05 ... 3.83582662e-05\n",
      "  3.83582662e-05 3.83582662e-05]]\n"
     ]
    }
   ],
   "source": [
    "def change_emission(emission):\n",
    "    \"\"\"change emission matrix according to prior info\n",
    "    para: emission matrix that waited to be changed\n",
    "    return: updated emission matrix\n",
    "    \"\"\"\n",
    "    tag = [tagset.get('USR'),tagset.get('HT'),\\\n",
    "               tagset.get('RT'),tagset.get('URL')]\n",
    "    word = [volcabulary.get('USER_TOKEN'),\\\n",
    "                volcabulary.get('HASHTAG_TOKEN'),\\\n",
    "                volcabulary.get('RETWEET_TOKEN'),\\\n",
    "                volcabulary.get('URL_TOKEN')]\n",
    "    chmx = emission\n",
    "    \n",
    "    # change emission probability from all special tag-> word 0\n",
    "    for i in tag:\n",
    "        for j in range(len(volcabulary)):\n",
    "            emission[i][j] = 0.0\n",
    "     \n",
    "    # change emission probability of all special tag->specific word 1\n",
    "    for (t,w) in zip(tag,word):\n",
    "        chmx[t,w] = 1.0\n",
    "       \n",
    "    return chmx\n",
    "\n",
    "\n",
    "emission = change_emission(O)\n",
    "print(emission)\n",
    "\n",
    "            \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now evaluate your new tagger on the test tweets again. You should report accuracy but also do a fine-grained error analysis. Print the F-scores for **each tag**. <b>Hint:</b> use the \"classification_report\" function in scikit-learn for that. You should report the tags that performed the best and the worse. (0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tagging test_corpus using updated emission matrix in viterbi\n",
    "prediction2 = tagging(test_corpus,pi,A,emission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6371419163648337\n",
      "0.6950938426078367\n",
      "\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.27      0.37      1159\n",
      "          1       0.85      1.00      0.92       303\n",
      "          2       0.59      0.59      0.59       268\n",
      "          3       0.43      0.54      0.48       393\n",
      "          4       0.64      0.59      0.61       670\n",
      "          5       0.53      0.97      0.69       181\n",
      "          6       0.65      0.70      0.68       660\n",
      "          7       0.74      0.93      0.82       825\n",
      "          8       0.79      0.63      0.70      1931\n",
      "          9       0.81      0.88      0.85      1091\n",
      "         10       0.72      0.83      0.77       875\n",
      "         11       0.69      0.78      0.73       342\n",
      "         12       0.88      0.50      0.64       303\n",
      "         13       0.96      0.88      0.92       305\n",
      "         14       0.77      0.74      0.75       306\n",
      "         15       0.43      0.63      0.51       140\n",
      "         16       0.00      0.00      0.00         2\n",
      "         17       0.71      0.76      0.73       680\n",
      "         18       0.84      0.96      0.90       264\n",
      "         19       0.86      0.82      0.84      1106\n",
      "         20       0.62      0.25      0.36        20\n",
      "         21       0.36      0.47      0.41        19\n",
      "         22       0.78      0.64      0.70       527\n",
      "         23       0.64      0.45      0.53       110\n",
      "         24       0.84      0.86      0.85       234\n",
      "         25       0.84      0.81      0.82        26\n",
      "         26       0.39      0.78      0.52        36\n",
      "         27       0.00      0.00      0.00         0\n",
      "         28       0.38      0.80      0.52        10\n",
      "         29       0.03      0.20      0.06        91\n",
      "         30       0.97      0.74      0.84        47\n",
      "         31       0.97      0.76      0.85       562\n",
      "         32       0.48      0.74      0.58        31\n",
      "         33       1.00      0.81      0.90       143\n",
      "         34       0.00      0.00      0.00         0\n",
      "         35       0.00      0.00      0.00         8\n",
      "         36       0.00      0.00      0.00         0\n",
      "         37       0.00      0.00      0.00        32\n",
      "         38       0.04      0.15      0.07        34\n",
      "         39       0.00      0.00      0.00         1\n",
      "         40       0.08      0.33      0.12         3\n",
      "         41       0.00      0.00      0.00         3\n",
      "         42       1.00      0.00      0.00       493\n",
      "         43       0.00      0.00      0.00        13\n",
      "         44       0.00      0.00      0.00         1\n",
      "         45       0.00      0.00      0.00         0\n",
      "         46       0.98      0.96      0.97       464\n",
      "         47       0.98      0.98      0.98       135\n",
      "         48       1.00      1.00      1.00       152\n",
      "         49       0.99      0.90      0.95       183\n",
      "         50       0.00      0.00      0.00         1\n",
      "         51       0.00      0.00      0.00         1\n",
      "         52       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.76      0.70      0.70     15185\n",
      "\n",
      "best performed tags:\n",
      "RT\n",
      "worst performed tags:\n",
      "['-NONE-', '``', '$', 'NNPS', 'WP$', '-LRB-', 'PDT', 'FW', 'UH', 'SYM', 'LS', '#', 'VPP', 'TD', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# extract second time pos tags\n",
    "pred_sequence2 = transform_prediction(prediction2)\n",
    "#accuracy2 = evaluation(gold_sequence, pred_sequence2)\n",
    "\n",
    "\n",
    "def print_metrices(y_test,y_pred_class):\n",
    "    \"\"\"pos report using scikit-learn classification report\n",
    "    para:y_test: gold standard labels\n",
    "    para:y_pred_class: predicted labels\n",
    "    \"\"\"\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "\n",
    "    \n",
    "print(metrics.accuracy_score(gold_sequence,pred_sequence))\n",
    "print(metrics.accuracy_score(gold_sequence,pred_sequence2))\n",
    "\n",
    "print_metrices(gold_sequence, pred_sequence2)\n",
    "\n",
    "print(\"best performed tags:\")\n",
    "print(inv_tagset[48])\n",
    "\n",
    "print(\"worst performed tags:\")\n",
    "worst = [inv_tagset[16], inv_tagset[27],\\\n",
    "        inv_tagset[34],inv_tagset[35],\\\n",
    "        inv_tagset[36],inv_tagset[37],\\\n",
    "        inv_tagset[39],inv_tagset[41],\\\n",
    "        inv_tagset[42],inv_tagset[43],\\\n",
    "        inv_tagset[44],inv_tagset[45],\\\n",
    "        inv_tagset[50],inv_tagset[51],\\\n",
    "        inv_tagset[52]]\n",
    "\n",
    "print(worst)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: finally, based on the information you got above, do some analysis. Why do you think the tagger performed worse on the tags you mentioned above? How would you improve the tagger? Feel free to inspect some instances manually if you want (and show us if you do). Write your analysis in the markdown cell below. Notice that this question is inherently subjective: this is on purpose as you will be evaluated on your analytical abilities. But don't worry about going into depth: 2-4 sentences is enough (but feel free to write more if you need). (0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WRITE YOUR ANALYSIS HERE</b>\n",
    "The first reason is the limitation on the amount of data that the classifier is not able to generalise well among rare instances in training set. However, we do get a good performance in tagging retweet token which also indicate a smaller variance in training data is able to result in a higher performance. Retweet token has only RT as tag whereas other words has more than one tags hold big variance such as nouns and verbs. A third reason is lack of prior information. In the first prediction, performance in most tags are relatively worse than the second prediction which contain some prior information. So combine this concern with the variants, we could find a way to maximise our expectation during tagging which could lower the variants. Another solution is simply apply a baseline method as it shows a 92% accuracy on WSJ corpus which might also have a higher accuracy in this data set."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
