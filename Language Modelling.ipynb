{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Jiyu Chen\n",
    "\n",
    "Student ID: 908066\n",
    "\n",
    "Python version used: 3.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: 11pm, Wednesday May 2nd\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character level language models, ranging from unigram approaches to higher over n-gram models. Your objective is to create an automatic player which makes the fewest mistakes.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hangman Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game, and a method allowing interactive play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowing better python 2 & python 3 compatibility \n",
    "from __future__ import print_function \n",
    "\n",
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of allowed mistakes\n",
    "        verbose: be chatty vs silent\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes\n",
    "\n",
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for manual play\n",
    "    \"\"\"\n",
    "    print('Enter your guess:')\n",
    "    try:\n",
    "        return raw_input().lower().strip() # python 3\n",
    "    except NameError:\n",
    "        return input().lower().strip() # python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hangman game. Target is _ _ _ _ _ _ _ _ length 8\n",
      "You have 8 attempts remaining.\n",
      "Enter your guess:\n",
      "f\n",
      "Guess is f\n",
      "Sorry, try again.\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "a\n",
      "Guess is a\n",
      "Good guess: _ _ a _ _ _ _ _\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "w\n",
      "Guess is w\n",
      "Good guess: w _ a _ _ _ _ _\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "h\n",
      "Guess is h\n",
      "Good guess: w h a _ _ _ _ _\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "t\n",
      "Guess is t\n",
      "Good guess: w h a t _ _ _ _\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "e\n",
      "Guess is e\n",
      "Good guess: w h a t e _ e _\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "v\n",
      "Guess is v\n",
      "Good guess: w h a t e v e _\n",
      "You have 7 attempts remaining.\n",
      "Enter your guess:\n",
      "r\n",
      "Guess is r\n",
      "Good guess: w h a t e v e r\n",
      "Congratulations, you won.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the method. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown`, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40234\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def corpusToWords():\n",
    "    \"\"\"preprocess the corpus and build a list of lexicons\n",
    "    return: partitioned lexicons as trainset isalpha\n",
    "    return: partitioned lexicons as testset\n",
    "    \"\"\"\n",
    "    lexicons = []\n",
    "    words = brown.words()\n",
    "    for w in words:\n",
    "        \n",
    "        word = w.lower()\n",
    "        if re.match(\"^[a-z]+$\",word):\n",
    "            lexicons.append(word)\n",
    "    lexicons = list(set(lexicons))\n",
    "    np.random.shuffle(lexicons)\n",
    "    \n",
    "    testset = lexicons[0:1000]\n",
    "    return lexicons, testset\n",
    "    \n",
    "trainset, testset = corpusToWords()\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To measure the performance of this (and later) techiques, implement a method that measures the average number of mistakes made by this technique over all the words in the `test_set`. You will want to turn off the printouts for this, using the `verbose=False` option, and increase the cap on the game length to `max_mistakes=26`. Print the average number of mistakes for the random AI, which will become a baseline for the following steps.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.694\n"
     ]
    }
   ],
   "source": [
    "def createAlpha():\n",
    "    \"\"\"create a list of 26 English alphabet\n",
    "    return: alphabet: a list of 26 English word\n",
    "    \"\"\"\n",
    "    alphabet = []\n",
    "    for letter in range(97,123):\n",
    "        alphabet.append(chr(letter))\n",
    "    return alphabet\n",
    "\n",
    "def ai(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for ai play\n",
    "    \"\"\"\n",
    "    alphabet = createAlpha()\n",
    "    try:\n",
    "        # randomly choose one alphabet which is\n",
    "        # not equals to the alphabets that have\n",
    "        # been chosed before\n",
    "        guess = np.random.choice(alphabet)\n",
    "        while guess in guessed:\n",
    "            guess = np.random.choice(alphabet)\n",
    "        return guess.strip() # python 3\n",
    "    except NameError:\n",
    "        return guess.strip() # python 2:\n",
    "    \n",
    "\n",
    "def AIGuess(ai):\n",
    "    \"\"\"play hangman via ai and calculate mistakes over all words\n",
    "    \"\"\"\n",
    "    mistakes = []\n",
    "    for word in testset:\n",
    "        mistake = 0\n",
    "        mistake += hangman(word, ai, 26, False)\n",
    "        mistakes.append(mistake)\n",
    "        \n",
    "    mis = evaluate(testset,mistakes)\n",
    "    print(mis)\n",
    "        \n",
    "def evaluate(testset,mistakes):\n",
    "    \"\"\"calculate mistakes over all words\n",
    "    para: testset: a set of lexicons as testset\n",
    "    para: mistakes: a list of mistakes on different lexicons\n",
    "    return: mistake_value: average mistake over all words\n",
    "    \"\"\"\n",
    "    return sum(mistakes)/len(testset)\n",
    "      \n",
    "AIGuess(ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guess function that returns the character with the highest probability, after aggregating (summing) the probability of each blank character in the secret word. Print the average number of mistakes the unigram method makes over the test set. Remember to exclude already guessed characters, and use the same evaluation method as above, so the results are comparable. (Hint: it should be much lower than for random).\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.434\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def convert_word(w):\n",
    "    \"\"\"add special start/end token to each lexicon\n",
    "    para: w: a lexicon\n",
    "    return: transformed lexicon\n",
    "    \"\"\"\n",
    "    return [\"<s>\"] + [alpha.lower() for alpha in w] + [\"</s>\"]\n",
    "\n",
    "def get_unigram_count(words):\n",
    "    \"\"\"count the existance of different alphabets over a list of words\n",
    "    para: words: a list of word which is used to perform alphabet count\n",
    "    return: number of existance of 28 alphabets(including start/end token)\n",
    "    \"\"\"\n",
    "    unigram_counts = Counter()\n",
    "    unigrams = Counter()\n",
    "\n",
    "    # collect initial unigram statistics\n",
    "    for word in words:\n",
    "        word = convert_word(word)\n",
    "        for alpha in word:\n",
    "            unigram_counts[alpha] += 1\n",
    "        #unigram_counts[\"</s>\"] += 1\n",
    "    return unigram_counts\n",
    "\n",
    "\n",
    "def get_unigram_prob(unigram_counts):\n",
    "    \"\"\"get unigram probability\n",
    "    para: unigram counts\n",
    "    return: unigram count probability and token count\n",
    "    \"\"\"\n",
    "    M = float(sum(unigram_counts.values()))\n",
    "    for alpha in unigram_counts:\n",
    "        unigram_counts[alpha] /= M\n",
    "    return unigram_counts,M\n",
    "    \n",
    "\n",
    "unigram_counts = get_unigram_count(trainset)\n",
    "unigrams,token_counts = get_unigram_prob(unigram_counts)\n",
    "\n",
    "\n",
    "def gram_pred_list(unigrams):\n",
    "    \"\"\"convert probability Counter to probability sorted list\n",
    "    para: unigrams: probability Counter\n",
    "    return: sorted unigram probability list\n",
    "    \"\"\"\n",
    "    x = unigrams\n",
    "    L = sorted(unigrams, key=unigrams.__getitem__, reverse=True)\n",
    "    try:\n",
    "        L.remove('<s>')\n",
    "        L.remove('</s>')\n",
    "    except:\n",
    "        pass\n",
    "    return L\n",
    "\n",
    "def ai_unigram(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for ai play\n",
    "    \"\"\"\n",
    "    \n",
    "    #convert unigram Counter to sorted list\n",
    "    predictions = gram_pred_list(unigrams)\n",
    "\n",
    "    # choose the most probable alphabet\n",
    "    guess = predictions[len(guessed)]\n",
    "    return guess.strip()\n",
    "    \n",
    "\n",
    "AIGuess(ai_unigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. Your job now is to incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having *different* unigram models for each length of word. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. Create another AI guessing function using this new model, and print its test performance.   \n",
    "\n",
    "(0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(unigram_counts)\n",
    "\n",
    "def length_count(words):\n",
    "    \"\"\"create a dictionary storing the total count of different length of words\n",
    "    para: words: a list of lexicons\n",
    "    return length_counts: a dictionary {length: total number of words under same length}\n",
    "    \"\"\"\n",
    "    length_counts = {}\n",
    "    for word in words:\n",
    "        # +2 indicate we will also consider start/end token\n",
    "        length_counts[len(word)+2] = length_counts.get(len(word)+2,0) +1\n",
    "    return length_counts\n",
    "\n",
    "# get c(length)\n",
    "length_counts = length_count(trainset)\n",
    "\n",
    "def get_counigram_counts(words):\n",
    "    \"\"\"count conditioning unigrams\n",
    "    para: words: trainset\n",
    "    return: conditioned unigram counts\n",
    "    \"\"\"\n",
    "    unigram_counts = defaultdict(Counter)\n",
    "    \n",
    "    # collect bigram counts\n",
    "    for word in words:\n",
    "        word = convert_word(word)\n",
    "        length = len(word)-2\n",
    "        for alpha in word:\n",
    "            unigram_counts[length][alpha] += 1           \n",
    "    return unigram_counts\n",
    "\n",
    "def get_con_unigrams(unigram_counts):\n",
    "    \"\"\"get conditioned unigram probability\n",
    "    para: unigram_counts\n",
    "    return: unigram probability\n",
    "    \"\"\"\n",
    "    \n",
    "    unigram_prob = defaultdict(Counter)\n",
    "    \n",
    "    for length in unigram_counts:\n",
    "        c = sum(unigram_counts[length].values())\n",
    "        for alpha in unigram_counts[length]:\n",
    "            unigram_prob[length][alpha] = unigram_counts[length][alpha] / c            \n",
    "    return unigram_prob\n",
    "\n",
    "grams = get_counigram_counts(trainset)\n",
    "condition_unigrams = get_con_unigrams(grams)\n",
    "\n",
    "\n",
    "def ai_condition_unigram(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for ai play\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(mask)            \n",
    "    possible = sorted(condition_unigrams[length], key=condition_unigrams[length].__getitem__, reverse=True)\n",
    "        \n",
    "    try:\n",
    "        # never predict special token\n",
    "        possible.remove('</s>')\n",
    "        possible.remove('<s>')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #robust unigram\n",
    "    padding = gram_pred_list(unigrams)\n",
    "                \n",
    "    try:\n",
    "        guess = possible[len(guessed)]\n",
    "        return guess.strip() # bigram\n",
    "    except:\n",
    "        #robust condition\n",
    "        padding.reverse()\n",
    "        guess = padding.pop()\n",
    "        while guess in guessed:\n",
    "            guess = padding.pop()\n",
    "        return guess\n",
    "                    \n",
    "AIGuess(ai_condition_unigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Now for the main challenge, using a *ngram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "Your job is to develop a *ngram* language model over characters, train this over the training words (being careful to handle the start of each word properly, e.g., by padding with sentinel symbols.) You should use linear interpolation to smooth between the higher order and lower order models, and you will have to decide how to weight each component. \n",
    "\n",
    "Your guessing AI algorithm should apply your language model to each blank position in the secret word by using as much of the left context as is known. E.g., in `_ e c _ e _ _` we know the full left context for the first blank (context=start of word), we have a context of two characters for the second blank (context=ec), one character for the second last blank (context=e), and no known context for the last one. If we were using a *n=3* order model, we would be able to apply it to the first and second blanks, but would only be able to use the bigram or unigram distributions for the subsequent blanks. As with the unigram model, you should sum over the probability distributions for each blank to find the expected count for each character type, then select the  character with the highest expected count.\n",
    "\n",
    "Implement the ngram method for *n=3,4* and *5* and evaluate each of these three models over the test set. Do you see any improvement over the unigram methods above?\n",
    "\n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.576\n"
     ]
    }
   ],
   "source": [
    "def get_bi_counts(words):\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    \n",
    "    # collect bigram counts\n",
    "    for word in words:\n",
    "        word = convert_word(word)\n",
    "        # generate a list of bigrams\n",
    "        bigram_list = zip(word[:-1], word[1:])\n",
    "        # iterate over bigrams\n",
    "        for bigram in bigram_list:\n",
    "            first, second = bigram\n",
    "            bigram_counts[first][second] += 1\n",
    "            \n",
    "    return bigram_counts\n",
    "\n",
    "def bigrams(bigram_counts):\n",
    "    \n",
    "    bigram_prob = defaultdict(Counter)\n",
    "    \n",
    "    for prev in bigram_counts:\n",
    "        c = sum(bigram_counts[prev].values())\n",
    "        for curr in bigram_counts[prev]:\n",
    "            bigram_prob[prev][curr] = bigram_counts[prev][curr] / c            \n",
    "    return bigram_prob\n",
    "\n",
    "bigram_counts = get_bi_counts(trainset)\n",
    "bigram_prob = bigrams(bigram_counts)\n",
    "\n",
    "def ai_bigram(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for ai play\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = list(mask)\n",
    "    \n",
    "    if mask[0] == '_':\n",
    "        prev = '<s>'\n",
    "    else:\n",
    "        for i in range(len(mask)):\n",
    "            if mask[i] == '_':\n",
    "                prev = mask[i-1]\n",
    "                break\n",
    "                \n",
    "    possible = sorted(bigram_prob[prev], key=bigram_prob[prev].__getitem__, reverse=False)\n",
    "    \n",
    "    try:\n",
    "        # never predict special token\n",
    "        possible.remove('</s>')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #robust unigram\n",
    "    padding = gram_pred_list(unigrams)\n",
    "                \n",
    "    try:\n",
    "        guess = possible.pop()\n",
    "        while guess in guessed:\n",
    "            guess = possible.pop()\n",
    "        return guess.strip() # bigram\n",
    "    except:\n",
    "        #robust condition\n",
    "        guess = padding.pop()\n",
    "        while guess in guessed:\n",
    "            guess = padding.pop()\n",
    "        return guess.strip()\n",
    "     \n",
    "AIGuess(ai_bigram)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ngram ai\n",
    "\n",
    "from nltk import ngrams\n",
    "import math\n",
    "\n",
    "words = trainset\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def get_ngram(n):\n",
    "    \"\"\"get_ngram_counts\n",
    "    para: n: ngram\n",
    "    return: counter format ngram counts\n",
    "    \"\"\"\n",
    "    ngram = defaultdict(Counter)\n",
    "    unigram = Counter()\n",
    "    for word in words:\n",
    "        # convert word\n",
    "        w = [\"<s4>\",\"<s3>\",\"<s2>\",\"<s1>\"] + [alpha.lower() for alpha in word] + [\"</s1>\",\"</s2>\",\"</s3>\",\"</s4>\"]\n",
    "        \n",
    "        # get ngrams\n",
    "        template = ngrams(w,n)\n",
    "        grams = [x for x in template]\n",
    "        \n",
    "        # save into Counter\n",
    "        for gram in grams:\n",
    "            prev = [x for x in gram]\n",
    "            #print(prev)\n",
    "            curr = prev.pop()\n",
    "            #print(tuple(prev))\n",
    "            if n > 2:\n",
    "                ngram[tuple(prev)][curr] += 1\n",
    "            elif n == 2:\n",
    "                ngram[prev.pop()][curr] += 1\n",
    "            elif n == 1:\n",
    "                unigram[curr] += 1\n",
    "    if n > 1:\n",
    "        return ngram\n",
    "    else:\n",
    "        return unigram\n",
    "\n",
    "\n",
    "# get unigrams counts\n",
    "unigram_counts = get_ngram(1)   \n",
    "# get bigram counts\n",
    "bigram_counts = get_ngram(2)\n",
    "# get tgram_counts\n",
    "tgram_counts = get_ngram(3)\n",
    "# get fgram_counts\n",
    "fgram_counts = get_ngram(4)\n",
    "# get figram_counts\n",
    "figram_counts = get_ngram(5)\n",
    "# get M\n",
    "token_counts = float(sum(unigrams.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.743\n"
     ]
    }
   ],
   "source": [
    "alphabet = createAlpha()\n",
    "\n",
    "#token_count = float(sum(unigrams.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_log_prob_interp(prev_word, word, unigram_counts, bigram_counts, trigram_counts, fourgram_counts, fivegram_counts, token_count, lambdas):\n",
    "    \"\"\"calculate log probability for each grams and return smoothing probability\n",
    "    para: prev_word: previous alphabets\n",
    "    para: word: current alphabets\n",
    "    para: ngram_counts\n",
    "    para: token_counts: total number of alphabets\n",
    "    para: lambdas: smoothing weights\n",
    "    return: smoothed sum of 1-5gram log probability\n",
    "    \"\"\"\n",
    "    fivegram_lambda = lambdas[0]\n",
    "    fourgram_lambda = lambdas[1]\n",
    "    trigram_lambda = lambdas[2]\n",
    "    bigram_lambda = lambdas[3]\n",
    "    unigram_lambda = lambdas[4]\n",
    "    #do not need zerogram in this case\n",
    "    \n",
    "    \n",
    "    # start by getting fivegram probability\n",
    "    sm_fivegram_counts = fivegram_counts[prev_word][word] * fivegram_lambda\n",
    "    if sm_fivegram_counts == 0.0:\n",
    "        interp_fivegram_counts = 0\n",
    "    else:\n",
    "        interp_fivegram_counts = sm_fivegram_counts / float(fourgram_counts[(prev_word[-4],prev_word[-3],prev_word[-2])][prev_word[-1]])\n",
    "    \n",
    "    # fourgram_probability\n",
    "    sm_fourgram_counts = fourgram_counts[prev_word][word] * fourgram_lambda\n",
    "    if sm_fourgram_counts == 0.0:\n",
    "        interp_fourgram_counts = 0\n",
    "    else:\n",
    "        interp_fourgram_counts = sm_fourgram_counts / float(trigram_counts[prev_word[-3]][prev_word[-2]][prev_word[-1]])\n",
    "    \n",
    "    \n",
    "    # trigram probability\n",
    "    sm_trigram_counts = trigram_counts[prev_word][word] * trigram_lambda\n",
    "    if sm_trigram_counts == 0.0:\n",
    "        interp_trigram_counts = 0\n",
    "    else:\n",
    "        interp_trigram_counts = sm_trigram_counts / float(bigram_counts[prev_word[-2]][prev_word[-1]])\n",
    "    \n",
    "    # bigram probability\n",
    "    sm_bigram_counts = bigram_counts[prev_word[-1]][word] * bigram_lambda\n",
    "    if sm_bigram_counts == 0.0:\n",
    "        interp_bigram_counts = 0\n",
    "    else:\n",
    "        interp_bigram_counts = sm_bigram_counts / float(unigram_counts[prev_word[-1]])\n",
    "     \n",
    "    # unigram probability\n",
    "    interp_unigram_counts = (unigram_counts[word]/token_counts) * unigram_lambda\n",
    "    \n",
    "    \n",
    "    # sum of 1-5gram log probability\n",
    "    prob = math.log(interp_fivegram_counts + interp_fourgram_counts + interp_trigram_counts + interp_bigram_counts + interp_unigram_counts)\n",
    "    return prob\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_word_log_prob_interp(prev, unigram_counts, bigram_counts, trigram_counts, fourgram_counts, fivegram_counts, token_count, lambdas):\n",
    "    \"\"\"given previous alphabets, generate a list of sorted probability of each alphabets\n",
    "    para: prev: previous alphabets\n",
    "    para: ngram_counts\n",
    "    para: token_counts: total number of alphabets in the trainset\n",
    "    para: lambdas: smoothing weight\n",
    "    return: a list of sorted tuple (probability, alphabet)\n",
    "    \"\"\"\n",
    "    prev_word = prev\n",
    "    pred = [(get_log_prob_interp(prev_word, \n",
    "                                    word, \n",
    "                                    unigram_counts, \n",
    "                                    bigram_counts,\n",
    "                                    trigram_counts,\n",
    "                                    fourgram_counts,\n",
    "                                    fivegram_counts,\n",
    "                                    token_count, \n",
    "                                    lambdas),word) for word in alphabet]\n",
    "    \n",
    "    pred = sorted(pred, key=lambda x: x[0])\n",
    "    return pred\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# trigram ai\n",
    "def trigram_ai(mask, guessed, **kwargs):\n",
    "    \"\"\"change lambdas to fit a trigram model\n",
    "    \"\"\"\n",
    "    mask = list(mask)\n",
    "    mask = ['<s4>','<s3>','<s2>','<s1>']+mask+['</s1>','</s2>','</s3>','</s4>']\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i] == '_':\n",
    "            prev = (mask[i-4],mask[i-3],mask[i-2],mask[i-1])\n",
    "            break\n",
    "    possible = get_word_log_prob_interp(prev, \n",
    "                                        unigram_counts, \n",
    "                                        bigram_counts, \n",
    "                                        tgram_counts, \n",
    "                                        fgram_counts, \n",
    "                                        figram_counts, \n",
    "                                        token_counts, \n",
    "                                        (0, 0,0.9,0.1,0.000001))   # trigram\n",
    "    pred = [x for y,x in possible]  \n",
    "    guess = pred.pop()\n",
    "    while guess in guessed or guess is \"<s4>\" or guess is \"<s3>\" or guess is \"<s2>\" or guess is \"<s1>\":\n",
    "        guess = pred.pop()\n",
    "    #print(guess)\n",
    "    return guess\n",
    "    \n",
    "AIGuess(trigram_ai)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.575\n"
     ]
    }
   ],
   "source": [
    "#fourgram ai\n",
    "def fourgram_ai(mask, guessed, **kwargs):\n",
    "    \"\"\"change lambdas to fit a fourgram model\n",
    "    \"\"\"\n",
    "    mask = list(mask)\n",
    "    mask = ['<s4>','<s3>','<s2>','<s1>']+mask+['</s1>','</s2>','</s3>','</s4>']\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i] == '_':\n",
    "            prev = (mask[i-4],mask[i-3],mask[i-2],mask[i-1])\n",
    "            break\n",
    "    possible = get_word_log_prob_interp(prev, \n",
    "                                        unigram_counts, \n",
    "                                        bigram_counts, \n",
    "                                        tgram_counts, \n",
    "                                        fgram_counts, \n",
    "                                        figram_counts, \n",
    "                                        token_counts, \n",
    "                                        (0, 0.9,0.1,0.01,0.00000000001))   # fourgram\n",
    "    pred = [x for y,x in possible]  \n",
    "    guess = pred.pop()\n",
    "    while guess in guessed or guess is \"<s4>\" or guess is \"<s3>\" or guess is \"<s2>\" or guess is \"<s1>\":\n",
    "        guess = pred.pop()\n",
    "    #print(guess)\n",
    "    return guess\n",
    "    \n",
    "AIGuess(fourgram_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.483\n"
     ]
    }
   ],
   "source": [
    "#fivegram ai\n",
    "#fourgram ai\n",
    "def fivegram_ai(mask, guessed, **kwargs):\n",
    "    \"\"\"change lambdas to fit a fivegram model\n",
    "    \"\"\"\n",
    "    mask = list(mask)\n",
    "    mask = ['<s4>','<s3>','<s2>','<s1>']+mask+['</s1>','</s2>','</s3>','</s4>']\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i] == '_':\n",
    "            prev = (mask[i-4],mask[i-3],mask[i-2],mask[i-1])\n",
    "            break\n",
    "    possible = get_word_log_prob_interp(prev, \n",
    "                                        unigram_counts, \n",
    "                                        bigram_counts, \n",
    "                                        tgram_counts, \n",
    "                                        fgram_counts, \n",
    "                                        figram_counts, \n",
    "                                        token_counts, \n",
    "                                        (0.9, 0.1,0.01,0.001,0.000001))   # fivegram\n",
    "    pred = [x for y,x in possible]  \n",
    "    guess = pred.pop()\n",
    "    while guess in guessed or guess is \"<s4>\" or guess is \"<s3>\" or guess is \"<s2>\" or guess is \"<s1>\":\n",
    "        guess = pred.pop()\n",
    "    #print(guess)\n",
    "    return guess\n",
    "    \n",
    "AIGuess(fivegram_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
