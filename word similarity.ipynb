{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "Python version used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: 1pm, Sunday April 1st\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be quantifying the similarity between pairs of words using the structure of WordNet and word co-occurrence in the Brown corpus, using PMI, LSA, and word2vec. You will quantify how well these methods work by comparing to a carefully filtered human annotated gold-standard.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> It is recommended to use Python 2 but we accept Python 3 solutions</b>. Make sure you state which version you used in the beggining of this notebook.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: For this homework we will be comparing our methods against a popular dataset of word similarities called Similarity-353. You need to first obtain this data set, which can be downloaded <a href=\"http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip\">here</a>. The file we will be using is called *combined.tab*. Except for the header (which should be stripped out), the file is tab formated with the first two columns corresponding to two words, and the third column representing a human-annotated similarity between the two words.\n",
    "\n",
    "Assume the file *combined.tab* is located <b>in the same folder as this notebook</b>. You should load this file into a Python dictionary (NOTE: in Python, tuples of strings, i.e. (\"tiger\",\"cat\") can serve as the keys of dictionaries). This dataset contains many rare words: we need to filter this dataset in order for it to be better suited to the resources we will use in this assignment. So your first goal is to filter this dataset to generate a smaller test set where you will evaluate your word similarity methods.\n",
    "\n",
    "The first filtering is based on document frequencies in the Brown corpus, in order to remove rare words. In this assignment, we will be treating the <i>paragraphs</i> of the Brown corpus as our \"documents\", you can iterate over them by using the `paras` method of the corpus reader. You should start by creating a Python list where each element of the list is a set containing the word <b>types</b> from a different paragraph of the Brown corpus: the words should be lower-cased and lemmatized before they are added to the set (keep it around, because you will need this list again later on). Then, using the information in this corpus, calculate document frequencies and remove from your test set any word pairs where at least one of the two words has a document frequency of less than 10 in this corpus. \n",
    "\n",
    "The second filtering is based on words with highly ambiguous senses and involves using the NLTK interface to WordNet. Here, you should remove any words which do not have a *single primary sense*. We define single primary sense here as either having only one sense (i.e. only one synset), or where the count (as provided by the WordNet `count()` method for the lemmas associated with a synset) of the most common sense is at least five and at least five times larger than the next most common sense. Also, you should remove any words where the primary sense is not a noun (this information is also in the synset). Store the synset corresponding to this primary sense in a dictionary for use in the next section. Given this definition, remove any word pairs from the test set where at least one of the words does not contain a single primary sense or if the single primary sense is not a noun.\n",
    "\n",
    "When you have applied these two filtering steps, print out all the pairs in your filtered test set (if you have done this correctly, the total should be more than 10, but less than 50).\n",
    "\n",
    "(1.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testset length after 1st filter:\n",
      "222\n",
      "second processed dataset length:\n",
      "29\n",
      "{('professor', 'doctor'): ['professor.n.01', 'doctor.n.01'], ('stock', 'egg'): ['stock.n.01', 'egg.n.01'], ('baby', 'mother'): ['baby.n.01', 'mother.n.01'], ('car', 'automobile'): ['car.n.01', 'car.n.01'], ('journey', 'voyage'): ['journey.n.01', 'ocean_trip.n.01'], ('coast', 'shore'): ['seashore.n.01', 'shore.n.01'], ('brother', 'monk'): ['brother.n.01', 'monk.n.01'], ('journey', 'car'): ['journey.n.01', 'car.n.01'], ('coast', 'hill'): ['seashore.n.01', 'hill.n.01'], ('monk', 'slave'): ['monk.n.01', 'slave.n.01'], ('coast', 'forest'): ['seashore.n.01', 'forest.n.01'], ('psychology', 'doctor'): ['psychology.n.01', 'doctor.n.01'], ('psychology', 'mind'): ['psychology.n.01', 'mind.n.01'], ('psychology', 'health'): ['psychology.n.01', 'health.n.01'], ('psychology', 'science'): ['psychology.n.01', 'science.n.01'], ('secretary', 'senate'): ['secretary.n.02', 'senate.n.01'], ('computer', 'laboratory'): ['computer.n.01', 'lab.n.01'], ('canyon', 'landscape'): ['canyon.n.01', 'landscape.n.01'], ('century', 'year'): ['century.n.01', 'year.n.01'], ('doctor', 'personnel'): ['doctor.n.01', 'force.n.04'], ('school', 'center'): ['school.n.01', 'center.n.01'], ('word', 'similarity'): ['word.n.01', 'similarity.n.01'], ('hotel', 'reservation'): ['hotel.n.01', 'reservation.n.01'], ('type', 'kind'): ['type.n.01', 'kind.n.01'], ('equipment', 'maker'): ['equipment.n.01', 'maker.n.01'], ('luxury', 'car'): ['luxury.n.01', 'car.n.01'], ('soap', 'opera'): ['soap.n.01', 'opera.n.01'], ('planet', 'people'): ['planet.n.01', 'people.n.01'], ('disaster', 'area'): ['calamity.n.01', 'area.n.01']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.corpus.reader.plaintext as reader\n",
    "from nltk.wsd import wordnet as wn\n",
    "\n",
    "dataset_path = \"/Users/alfredgordon/Downloads/wordsim353/combined.tab\"\n",
    "head = str('Word 1\\tWord 2\\tHuman (mean)\\n')\n",
    "\n",
    "\n",
    "\n",
    "def con_dic(dataset_path):\n",
    "    \"\"\"construct dictionary as requirement\n",
    "    para: dataet_path: combined.tab file path\n",
    "    return: dicionary format combined.tab\n",
    "    \"\"\"\n",
    "    dataset = open(dataset_path, \"r\")\n",
    "    next(dataset)\n",
    "    testset = {}\n",
    "    #key = (word1,word2) word pairs in combined.tag\n",
    "    for l in dataset:\n",
    "        \n",
    "            k1 = re.sub(r'^([a-zA-Z]+)\t[a-zA-Z]+\t[0-9.]+$',r'\\1',l)\n",
    "            k2 = re.sub(r'^[a-zA-Z]+\t([a-zA-Z]+)\t[0-9.]+$',r'\\1',l)\n",
    "            key = (k1.strip('\\n'),k2.strip('\\n'))\n",
    "            #value = '0.0'\n",
    "            value = re.sub(r'^[a-zA-Z]+\t[a-zA-Z]+\t([0-9.]+)$',r'\\1',l)\n",
    "            testset[key] = float(value)\n",
    "    return testset\n",
    "\n",
    "def doc_setup(dictionary):\n",
    "    \"\"\"parse brown corpus by paragraphs\n",
    "    para: dictionary: testset\n",
    "    return: parsed brown corpus as documents\n",
    "    \"\"\"\n",
    "    ds = []\n",
    "    paras = brown.paras()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for doc in paras:\n",
    "        d = []\n",
    "        for sent in doc:\n",
    "            for word in sent:\n",
    "                w = wordnet_lemmatizer.lemmatize(word.lower())\n",
    "                d.append(w)\n",
    "        ds.append(set(d))  \n",
    "    return ds\n",
    "\n",
    "def doc_frequency(w, docs):\n",
    "    \"\"\"count document frequency for individual word\n",
    "    paras: w: word\n",
    "    paras: docs: a list of documents\n",
    "    return: document frequency count\n",
    "    \"\"\"\n",
    "    df = 0\n",
    "    for p in docs:\n",
    "        if w in p:\n",
    "            df += 1\n",
    "    return df\n",
    "\n",
    "def first_filter(dictionary, remove_degree):\n",
    "    \"\"\"delete word pairs from dictionary if\n",
    "    one of their document frequency is less than\n",
    "    remove_degree\n",
    "    para: dictionary: dictionary format of word-pairs\n",
    "    para: remove_degree: default to be 10\n",
    "    return: filtered dictionary, a list of documents\n",
    "    \"\"\"\n",
    "    docs = doc_setup(dictionary)\n",
    "    for (k1,k2) in list(dictionary):\n",
    "        if doc_frequency(k1,docs) < remove_degree or \\\n",
    "        doc_frequency(k2,docs) < remove_degree:\n",
    "            del dictionary[(k1,k2)]            \n",
    "    print(\"testset length after 1st filter:\")\n",
    "    print(len(dictionary))\n",
    "    return dictionary, docs\n",
    "        \n",
    "\n",
    "def second_max(a):\n",
    "    \"\"\"find second max in a list\n",
    "    para: a: list of number\n",
    "    return: second max number\n",
    "    \"\"\"\n",
    "    hi = mid = 0\n",
    "    for x in a:\n",
    "        if x > hi:\n",
    "            mid = hi\n",
    "            hi = x\n",
    "        elif x < hi and x > mid:\n",
    "            lo = mid\n",
    "            mid = x\n",
    "    return mid\n",
    "\n",
    "def primary_filter(word, filter_degree):\n",
    "    \"\"\"check primary synset\n",
    "    paras: word that need to be checked\n",
    "    paras: filter_degree: lemma times bigger\n",
    "    return None of suitable primary synset\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word)\n",
    "    # only one synset\n",
    "    if len(synsets) == 1:\n",
    "        if synsets[0].pos() != 'n':\n",
    "            return None\n",
    "        else:\n",
    "            return synsets[0].name()\n",
    "    #more than one synset\n",
    "    elif len(synsets) > 1:\n",
    "        return lemma_filter(word, filter_degree)\n",
    "\n",
    "\n",
    "def lemma_filter(word, filter_degree):\n",
    "    \"\"\"check primary synset based on lemma count\n",
    "    paras: word\n",
    "    paras: filter_degree: lemma count times\n",
    "    return None or suitable primary synset\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word)\n",
    "    count = {}\n",
    "    for index,s in enumerate(synsets):\n",
    "        #record compare most common n next most common       \n",
    "        c = 0            \n",
    "        for lemma in s.lemmas():\n",
    "            lemma_name = lemma.name()\n",
    "            if lemma_name == word:\n",
    "                    #print(word)\n",
    "                c = lemma.count()\n",
    "                count[c] = index\n",
    "    #print(count)\n",
    "    secmax = second_max(list(count))\n",
    "    c = max(list(count))\n",
    "\n",
    "    if synsets[count.get(c)].pos() == 'n':\n",
    "        #at least five or five times bigger\n",
    "        if c >= filter_degree and c >= filter_degree * secmax:\n",
    "            return synsets[count.get(c)].name()  \n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def second_filter(dictionary, filter_degree, add_synset):\n",
    "    \"\"\"filter out not noun words without suitable primary synset\n",
    "    para: dictionary: dictionary format of word-pairs\n",
    "    para: filter_degree: control lemma count default 5\n",
    "    para: add_synset: create (wordpair):synset dictionary or not\n",
    "    return: filtered dictionary    \n",
    "    \"\"\"\n",
    "    for (k1,k2) in list(dictionary):\n",
    "        f1 = primary_filter(k1,filter_degree)\n",
    "        f2 = primary_filter(k2,filter_degree)\n",
    "        if f1 and f2:\n",
    "                synsets = [f1,f2]\n",
    "                if add_synset is True:\n",
    "                    dictionary[(k1,k2)] = synsets                    \n",
    "        else:\n",
    "                del dictionary[(k1,k2)]\n",
    "    if add_synset is True:\n",
    "        print(\"second processed dataset length:\")\n",
    "        print(len(dictionary))\n",
    "        print(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "\n",
    "testset_dic = con_dic(dataset_path)\n",
    "testset_dic, docs = first_filter(testset_dic, 10)\n",
    "\n",
    "#primary_filter(\"eat\")\n",
    "testset_dic = second_filter(testset_dic, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now you will create several dictionaries with similarity scores for pairs of words in your test set derived using the techniques discussed in class. The first of these is the Wu-Palmer scores derived from the hypernym relationships in WordNet, which you should calculate using the primary sense for each word derived above. You can use the built-in method included in the NLTK interface, you don't have to implement your own. When you're done,  print out the Python dictionary of word pair/similarity mappings. \n",
    "\n",
    "(0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.5, ('stock', 'egg'): 0.11764705882352941, ('baby', 'mother'): 0.5, ('car', 'automobile'): 1.0, ('journey', 'voyage'): 0.8571428571428571, ('coast', 'shore'): 0.9090909090909091, ('brother', 'monk'): 0.5714285714285714, ('journey', 'car'): 0.09523809523809523, ('coast', 'hill'): 0.6666666666666666, ('monk', 'slave'): 0.6666666666666666, ('coast', 'forest'): 0.16666666666666666, ('psychology', 'doctor'): 0.1111111111111111, ('psychology', 'mind'): 0.5714285714285714, ('psychology', 'health'): 0.21052631578947367, ('psychology', 'science'): 0.9411764705882353, ('secretary', 'senate'): 0.13333333333333333, ('computer', 'laboratory'): 0.35294117647058826, ('canyon', 'landscape'): 0.3333333333333333, ('century', 'year'): 0.8333333333333334, ('doctor', 'personnel'): 0.13333333333333333, ('school', 'center'): 0.13333333333333333, ('word', 'similarity'): 0.3333333333333333, ('hotel', 'reservation'): 0.375, ('type', 'kind'): 0.9473684210526315, ('equipment', 'maker'): 0.5, ('luxury', 'car'): 0.1111111111111111, ('soap', 'opera'): 0.2222222222222222, ('planet', 'people'): 0.18181818181818182, ('disaster', 'area'): 0.14285714285714285}\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "def WP_scores(dictionary):\n",
    "    \"\"\"compare similarity of word pairs based on Wu-Palmer similarity\n",
    "    paras: dictionary: dictionary format of word-pairs\n",
    "    return: a dictionary (wordpair):similarity score\n",
    "    \"\"\"\n",
    "    similarity_dict = {}    \n",
    "    for (k1,k2) in list(dictionary):\n",
    "        synsets = dictionary.get((k1,k2))\n",
    "        \n",
    "        w1 = wn.synset(synsets[0])\n",
    "        w2 = wn.synset(synsets[1])\n",
    "        \n",
    "        score=w1.wup_similarity(w2)\n",
    "        #print(score)\n",
    "        \n",
    "        similarity_dict[(k1,k2)] = score\n",
    "    print(similarity_dict)\n",
    "    return similarity_dict\n",
    "    \n",
    "        \n",
    "    \n",
    "WP_sim = WP_scores(testset_dic)\n",
    "print(len(testset_dic))\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Next, you will calculate Positive PMI (PPMI) for your word pairs using statistics derived from the Brown: you should use the same set up as you did to calculate document frequency above: paragraphs as documents, lemmatized, lower-cased, and with term frequency information removed by conversion to Python sets. You need to use the basic method for calculating PPMI introduced in class (and also in the reading) which is appropriate for any possible definition of co-occurrence (here, appearing in the same paragraph), but you should only calculate PPMI for the words in your test set. You must avoid building the entire co-occurrence matrix, instead you should keeping track of the sums you need for the probabilities as you go along. When you have calculated PMI for all the pairs, your code should print out the Python dictionary of word-pair/PPMI-similarity mappings.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.0, ('stock', 'egg'): 7.432658852069735, ('baby', 'mother'): 8.722036679055751, ('car', 'automobile'): 8.900113284110693, ('journey', 'voyage'): 0.0, ('coast', 'shore'): 10.245932998315858, ('brother', 'monk'): 8.514452943233382, ('journey', 'car'): 0.0, ('coast', 'hill'): 6.828245920623065, ('monk', 'slave'): 0.0, ('coast', 'forest'): 8.665692907837105, ('psychology', 'doctor'): 9.177761495674279, ('psychology', 'mind'): 8.394859617341213, ('psychology', 'health'): 0.0, ('psychology', 'science'): 10.693682351965784, ('secretary', 'senate'): 9.611782098320695, ('computer', 'laboratory'): 0.0, ('canyon', 'landscape'): 0.0, ('century', 'year'): 6.470397157835755, ('doctor', 'personnel'): 7.833807094456917, ('school', 'center'): 6.359230800285396, ('word', 'similarity'): 0.0, ('hotel', 'reservation'): 8.506232436428414, ('type', 'kind'): 6.265260462553219, ('equipment', 'maker'): 9.898498628048598, ('luxury', 'car'): 7.887513247331061, ('soap', 'opera'): 9.836381038120743, ('planet', 'people'): 6.024433004762661, ('disaster', 'area'): 5.885512745451623}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def PPMI(word1, word2, documents):\n",
    "    \"\"\"window size will be one paragraph in brown corpus\n",
    "    compare similarity of a word pair based on PPMI\n",
    "    paras: word1, word2: word pair that waiting to be compared\n",
    "    paras: documents: a list of document\n",
    "    return: PPMI value        \n",
    "    \"\"\"\n",
    "    w1_count = 0\n",
    "    w2_count = 0\n",
    "    total_count = 0\n",
    "    both_count = 0\n",
    "    #window = window_size //2\n",
    "    for doc in documents:\n",
    "        for i in doc:\n",
    "            total_count += 1\n",
    "            if i == word1:\n",
    "                w1_count += 1\n",
    "                for w in doc:\n",
    "                    if w == word2:\n",
    "                        both_count += 1\n",
    "            elif i == word2:\n",
    "                w2_count += 1\n",
    "            \n",
    "    base = (both_count/total_count)/((w1_count/total_count)*(w2_count/total_count))    \n",
    "    if base > 0:\n",
    "        PMI = math.log((both_count/total_count)/((w1_count/total_count)*(w2_count/total_count)), 2)\n",
    "        #print(PMI)\n",
    "        return PMI\n",
    "    else:\n",
    "        #print(0.0)\n",
    "        return float(0)\n",
    "    \n",
    "def dic_PPMI(dictionary, docs):\n",
    "    \"\"\"compare similarity of word pairs based on Wu-Palmer similarity\n",
    "    paras: dictionary: dictionary format of word-pairs\n",
    "    paras: docs: a list of documents\n",
    "    return: a dictionary (wordpair):PPMI \n",
    "    \"\"\"\n",
    "    pmi_dic = {}\n",
    "    for (k1,k2) in list(dictionary):\n",
    "        pmi = PPMI(k1,k2,docs)\n",
    "        pmi_dic[(k1,k2)] = pmi\n",
    "    print(pmi_dic)\n",
    "    return pmi_dic\n",
    "            \n",
    "PPMI_sim = dic_PPMI(testset_dic, docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Next, you will derive similarity scores using the LSA method, i.e. apply SVD and truncate to get a dense vector and then use cosine similarity between the two vectors for each word pair. You can use the Distributed Semantics notebook as a starting point, but note that since you are interested here in word semantics, you will be constructing a matrix where the (non-sparse) rows correspond to words in the vocabulary, and the (sparse) columns correspond to the texts where they appear (this is the opposite of the notebook). Again, use the Brown corpus, in the same format as with PMI and document frequency. After you have a matrix in the correct format, use truncatedSVD in Sci-kit learn to produce dense vectors of length 500, and then use cosine similarity to produce similarities for your word pairs. Print out the corresponding Python dictionary.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.07962727332873365, ('stock', 'egg'): 0.1410014992605253, ('baby', 'mother'): 0.32308500443762656, ('car', 'automobile'): 0.36675608829519946, ('journey', 'voyage'): 0.12580095162843796, ('coast', 'shore'): 0.3838696011131211, ('brother', 'monk'): 0.06057832286592235, ('journey', 'car'): 0.013095584487878176, ('coast', 'hill'): 0.19187191501604195, ('monk', 'slave'): -0.04944490856392281, ('coast', 'forest'): 0.1020773221215131, ('psychology', 'doctor'): 0.11453583496975728, ('psychology', 'mind'): 0.11621736949500316, ('psychology', 'health'): 0.023813308889068074, ('psychology', 'science'): 0.28523511157689485, ('secretary', 'senate'): 0.39805789096150535, ('computer', 'laboratory'): 0.13323797796412695, ('canyon', 'landscape'): 0.11497281137165394, ('century', 'year'): 0.06781879547079739, ('doctor', 'personnel'): 0.023514115940761693, ('school', 'center'): 0.046829530650918216, ('word', 'similarity'): -0.003605270742967459, ('hotel', 'reservation'): 0.07459300597913754, ('type', 'kind'): 0.038891752815024994, ('equipment', 'maker'): 0.2583629271417873, ('luxury', 'car'): 0.09431562581286501, ('soap', 'opera'): 0.000402489265157735, ('planet', 'people'): 0.03180386909780908, ('disaster', 'area'): 0.005529506753467528}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#code below is refered from WSTA_N4_distributional_semantics\n",
    "\n",
    "def get_BOW(text):\n",
    "    \"\"\"bag-of-words feature selection\n",
    "    para: text: one piece of document\n",
    "    return: dictionary format of bag-of-words\n",
    "    \"\"\"\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "\n",
    "#end of reference\n",
    "\n",
    "\n",
    "\n",
    "def cons_feature_matrix(docs):\n",
    "    \"\"\"create truncatedSVD matrix for word-document feature\n",
    "    para: docs: a list of documents\n",
    "    return: created feature_matrix, dictionary_vectorizer\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        texts.append(get_BOW(doc))\n",
    "                            \n",
    "    vectorizer = DictVectorizer()\n",
    "    #Tfidf is optional\n",
    "    transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "    svd = TruncatedSVD(n_components=500)\n",
    "    \n",
    "    # create truncated transposed document-word feature matrix \n",
    "    # as word-doc feature matrix\n",
    "    feature_matrix = svd.fit_transform(vectorizer.fit_transform(texts).T)\n",
    "    return feature_matrix,vectorizer\n",
    "\n",
    "\n",
    "def cos_sim(testset,docs,feature_matrix, vectorizer):\n",
    "    \"\"\"compare pairs of word similary based on cosine distance\n",
    "       and TruncatedSVD\n",
    "    paras: testset: dictionary format of word pairs\n",
    "    paras: a list of documents\n",
    "    paras: feature_matrix: truncated word-doc matrix\n",
    "    paras: vectorizer: a BOW vectorizer for getting word index\n",
    "    return: dictionary format as (wordpairs):cosine similarity\n",
    "    \"\"\"\n",
    "    look_up = {}\n",
    "    cos_dic = {}\n",
    "    for index,w in enumerate(vectorizer.get_feature_names()):\n",
    "        look_up[w]=index\n",
    "    \n",
    "    for (k1,k2) in list(testset):\n",
    "        v1 = look_up.get(k1)\n",
    "        v2 = look_up.get(k2)\n",
    "        cos = cos_distance(feature_matrix[v1],feature_matrix[v2])\n",
    "        # cosine_similary = 1-cosine_distance\n",
    "        cos_dic[(k1,k2)]=1-cos        \n",
    "    print(cos_dic)\n",
    "    return cos_dic\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "feature_matrix,vectorizer = cons_feature_matrix(docs)\n",
    "cos_sim = cos_sim(testset_dic,docs,feature_matrix, vectorizer)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Instructions:** Next, you will derive a similarity score from word2vec vectors, using the Gensim interface. Check the Gensim word2vec tutorial for details on the API: https://radimrehurek.com/gensim/models/word2vec.html. Again, you should use the Brown for this, but for word2vec you don't need to worry about paragraphs: feel free to train your model at the sentence level instead. Your vectors should have the same number of dimensions as LSA (500), and you need to run for 50 iterations. This may take a while (several minutes), but that's okay, you won't be marked based on the speed of this. You should extract the similarites you need directly from the Gensim model, put them in a Python dictionary, and print them out.\n",
    "\n",
    "(0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.10521473964870218, ('stock', 'egg'): 0.1485685681150803, ('baby', 'mother'): 0.23632185915564013, ('car', 'automobile'): 0.17277914140820544, ('journey', 'voyage'): 0.47617062637383206, ('coast', 'shore'): 0.4051329414318604, ('brother', 'monk'): 0.04042875805562679, ('journey', 'car'): 0.19801697248395853, ('coast', 'hill'): 0.44527485444604537, ('monk', 'slave'): 0.013772762772213785, ('coast', 'forest'): 0.2879094782531423, ('psychology', 'doctor'): -0.03753880586464503, ('psychology', 'mind'): 0.046343553734534054, ('psychology', 'health'): 0.16924622535064146, ('psychology', 'science'): 0.30491753025731005, ('secretary', 'senate'): 0.09820516122337106, ('computer', 'laboratory'): 0.18315515677738242, ('canyon', 'landscape'): 0.16869738104739962, ('century', 'year'): 0.30330282933877273, ('doctor', 'personnel'): -0.05721843137511085, ('school', 'center'): -0.04422633297388947, ('word', 'similarity'): 0.0383974812192422, ('hotel', 'reservation'): 0.05318689225659905, ('type', 'kind'): 0.27178489035256975, ('equipment', 'maker'): 0.1826096920865032, ('luxury', 'car'): 0.1450908154785386, ('soap', 'opera'): -0.030010058557197447, ('planet', 'people'): -0.01437412563537366, ('disaster', 'area'): 0.09390081659850595}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_w2v():\n",
    "    \"\"\"train word2vec model in sentence level\n",
    "    return:trained model\n",
    "    \"\"\"\n",
    "    sentences = brown.sents()   \n",
    "    model = Word2Vec(sentences, size=500, iter=50)\n",
    "    return model\n",
    "\n",
    "def w2v_sim_w(word1, word2, model):\n",
    "    \"\"\"compare similary of two words\n",
    "    para: word1,word2\n",
    "    para: model\n",
    "    return: similarity score\n",
    "    \"\"\"\n",
    "    similarity = model.wv.similarity(word1, word2)\n",
    "    #print(similarity)\n",
    "    return similarity\n",
    "    \n",
    "    \n",
    "def w2v_sim(dataset, model):\n",
    "    w2v_sim = {}\n",
    "    for (k1,k2) in list(dataset):\n",
    "        sim = w2v_sim_w(k1,k2,model)\n",
    "        w2v_sim[(k1,k2)] = sim\n",
    "    print(w2v_sim)\n",
    "    return w2v_sim\n",
    "        \n",
    "    \n",
    "model = train_w2v()    \n",
    "W2V = w2v_sim(testset_dic, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Instructions:** Finally, you should compare all the similarities you've created to the gold standard you loaded and filtered in the first step. For this, you can use the Pearson correlation co-efficient (`pearsonr`), which is included in scipy (`scipy.stats`). Be careful converting your dictionaries to lists for this purpose, the data for the two datasets needs to be in the same order for correct comparison using correlation. Write a general function, then apply it to each of the similarity score dictionaries, and print out the result for each (be sure to label them!). Hint: All of the methods used here should be markedly above 0, but also far from 1 (perfect correlation); if you're not getting reasonable results, go back and check your code for bugs!  \n",
    "\n",
    "(0.5 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testset length after 1st filter:\n",
      "222\n",
      "wu-Palmer:\n",
      "(0.4591586058919407, 0.012225751338546089)\n",
      "PPMI:\n",
      "(0.10537972921158244, 0.5864114263344311)\n",
      "Cosine:\n",
      "(0.2866665326664514, 0.1316342766703332)\n",
      "word2vector:\n",
      "(0.3203661168617215, 0.09020361755065244)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "ori_testset = con_dic(dataset_path)\n",
    "ori_test, docs = first_filter(ori_testset, 10)\n",
    "#primary_filter(\"eat\")\n",
    "testset_tem = second_filter(ori_testset,5,False)\n",
    "\n",
    "\n",
    "\n",
    "def dic2list(dic1, dic2):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for (k1,k2) in list(dic1):\n",
    "        l1.append(dic1.get((k1,k2)))\n",
    "        l2.append(dic2.get((k1,k2)))\n",
    "    return l1,l2\n",
    "\n",
    "\n",
    "\n",
    "olist, WPlist = dic2list(ori_testset, WP_sim)\n",
    "#print(olist)\n",
    "#print(WPlist)\n",
    "olist,PPMIlist = dic2list(ori_testset,PPMI_sim)\n",
    "olist,COSlist = dic2list(ori_testset,cos_sim)\n",
    "olist,W2Vlist = dic2list(ori_testset, W2V)\n",
    "print(\"wu-Palmer:\")\n",
    "print(stats.pearsonr(olist, WPlist))\n",
    "print(\"PPMI:\")\n",
    "print(stats.pearsonr(olist, PPMIlist))\n",
    "print(\"Cosine:\")\n",
    "print(stats.pearsonr(olist,COSlist))\n",
    "print(\"word2vector:\")\n",
    "print(stats.pearsonr(olist,W2Vlist))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final word\n",
    "\n",
    "Normally, we would not use a corpus as small as the Brown for the purposes of building distributional word vectors. Also, note that filtering our test set to just words we are likely to do well on would typically be considered cheating."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
